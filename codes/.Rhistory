library(selectr)
library(rvest)
library(xml2)
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library(tm) # for text minning
library(dplyr) # loads of fun stuff including piping
library(ROAuth)
library(jsonlite)
library(httpuv)
### Setting API parameters
api = read.csv('Twitter_API.txt')
consumer_key = api[api["Type"]=="API Key"][2]
consumer_secret = api[api["Type"]=="API Key Secret"][2]
access_token = api[api["Type"]=="Access Token"][2]
access_token_secret = api[api["Type"]=="Access Token Secret"][2]
bearer_token = api[api["Type"]=="Bearer Token"][2]
# print(consumer_key)
# print(consumer_secret)
# print(access_token)
# print(access_token_secret)
# print(bearer_token)
### Extracting tweets
library(twitteR)
requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
s_key  = 'DEI'
n_tweets = 250
twitteR:::setup_twitter_oauth(consumer_key, consumer_secret,access_token,access_token_secret)
library(selectr)
library(rvest)
library(xml2)
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library(tm) # for text minning
library(dplyr) # loads of fun stuff including piping
library(ROAuth)
library(jsonlite)
library(httpuv)
### Setting API parameters
api = read.csv('Twitter_API.txt')
consumer_key = api[api["Type"]=="API Key"][2]
consumer_secret = api[api["Type"]=="API Key Secret"][2]
access_token = api[api["Type"]=="Access Token"][2]
access_token_secret = api[api["Type"]=="Access Token Secret"][2]
bearer_token = api[api["Type"]=="Bearer Token"][2]
# print(consumer_key)
# print(consumer_secret)
# print(access_token)
# print(access_token_secret)
# print(bearer_token)
### Extracting tweets
library(twitteR)
requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
s_key  = 'DEI'
n_tweets = 250
twitteR:::setup_twitter_oauth(consumer_key, consumer_secret,access_token,access_token_secret)
Search1<-twitteR::searchTwitter(s_key,n=n_tweets, since="2022-04-01")
# Search2<-twitteR::searchTwitter("#datascience",n=50, since="2022-01-01")
TweetsDF<- twListToDF(Search1)
#(TweetsDF$text[1])
### Saving the tweets to file
########## Place Tweets in a new file ###################
FName = "./data/Tweets.txt"
## Start the file
MyFile <- file(FName)
## Write Tweets to file
cat(unlist(TweetsDF), " ", file=MyFile, sep="\n")
library(selectr)
library(rvest)
library(xml2)
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library(tm) # for text minning
library(dplyr) # loads of fun stuff including piping
library(ROAuth)
library(jsonlite)
library(httpuv)
### Setting API parameters
api = read.csv('Twitter_API.txt')
consumer_key = api[api["Type"]=="API Key"][2]
consumer_secret = api[api["Type"]=="API Key Secret"][2]
access_token = api[api["Type"]=="Access Token"][2]
access_token_secret = api[api["Type"]=="Access Token Secret"][2]
bearer_token = api[api["Type"]=="Bearer Token"][2]
# print(consumer_key)
# print(consumer_secret)
# print(access_token)
# print(access_token_secret)
# print(bearer_token)
### Extracting tweets
library(twitteR)
requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
s_key  = 'DEI'
n_tweets = 250
twitteR:::setup_twitter_oauth(consumer_key, consumer_secret,access_token,access_token_secret)
Search1<-twitteR::searchTwitter(s_key,n=n_tweets, since="2022-04-01")
# Search2<-twitteR::searchTwitter("#datascience",n=50, since="2022-01-01")
TweetsDF<- twListToDF(Search1)
#(TweetsDF$text[1])
### Saving the tweets to file
########## Place Tweets in a new file ###################
FName = "../data/Tweets.txt"
## Start the file
MyFile <- file(FName)
## Write Tweets to file
cat(unlist(TweetsDF), " ", file=MyFile, sep="\n")
close(MyFile)
library(selectr)
library(rvest)
library(xml2)
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library(tm) # for text minning
library(dplyr) # loads of fun stuff including piping
library(ROAuth)
library(jsonlite)
library(httpuv)
### Setting API parameters
api = read.csv('Twitter_API.txt')
consumer_key = api[api["Type"]=="API Key"][2]
consumer_secret = api[api["Type"]=="API Key Secret"][2]
access_token = api[api["Type"]=="Access Token"][2]
access_token_secret = api[api["Type"]=="Access Token Secret"][2]
bearer_token = api[api["Type"]=="Bearer Token"][2]
# print(consumer_key)
# print(consumer_secret)
# print(access_token)
# print(access_token_secret)
# print(bearer_token)
### Extracting tweets
library(twitteR)
requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
s_key  = 'inclusive'
n_tweets = 250
twitteR:::setup_twitter_oauth(consumer_key, consumer_secret,access_token,access_token_secret)
Search1<-twitteR::searchTwitter(s_key,n=n_tweets, since="2022-04-01")
# Search2<-twitteR::searchTwitter("#datascience",n=50, since="2022-01-01")
TweetsDF<- twListToDF(Search1)
#(TweetsDF$text[1])
### Saving the tweets to file
########## Place Tweets in a new file ###################
FName = "../data/Tweets.txt"
## Start the file
MyFile <- file(FName)
## Write Tweets to file
cat(unlist(TweetsDF), " ", file=MyFile, sep="\n")
close(MyFile)
knitr::opts_chunk$set(echo = TRUE)
# Load packages
library('tidyverse')
library('ggplot2')
library('rstudioapi')
# Set directory to data folder
cur_dir <- dirname(getSourceEditorContext()$path)
setwd(cur_dir)
setwd("../data")
# Read csv
df <- read_csv('raw_wf_demo.csv')
# 1. rcid column
# Remove since we don't need company id for only 6 companies
df <- subset(df, select = -c(rcid))
# 2. company column
# Remove ', Inc.'
df$company <- gsub(
pattern = (', Inc.'),
replacement = '',
as.character(df$company))
# Remove '\\'
df$company <- gsub(
pattern = ('\\\\'),
replacement = '',
as.character(df$company))
# 3. region column
# Fill empty values with NA
df$region <- gsub(
pattern = ('empty'),
replacement = NA,
as.character(df$region))
# 4. year column
# change 'month' column to 'year'
df$year = substr(df$month,1,nchar(df$month)-3)
df <- subset(df, select = -c(month))
# Load packages
library('tidyverse')
library('ggplot2')
library('rstudioapi')
# Set directory to data folder
cur_dir <- dirname(getSourceEditorContext()$path)
setwd(cur_dir)
setwd("../data")
# Read csv
df <- read_csv('raw_wf_demo.csv')
# 1. rcid column
# Remove since we don't need company id for only 6 companies
df <- subset(df, select = -c(rcid))
# 2. company column
# Remove ', Inc.'
df$company <- gsub(
pattern = (', Inc.'),
replacement = '',
as.character(df$company))
# Remove '\\'
df$company <- gsub(
pattern = ('\\\\'),
replacement = '',
as.character(df$company))
# 3. region column
# Fill empty values with NA
df$region <- gsub(
pattern = ('empty'),
replacement = NA,
as.character(df$region))
# 4. year column
# change 'month' column to 'year'
df$year = substr(df$month,1,nchar(df$month)-3)
df <- subset(df, select = -c(month))
# Load packages
library('tidyverse')
library('ggplot2')
library('rstudioapi')
# Set directory to data folder
cur_dir <- dirname(getSourceEditorContext()$path)
setwd(cur_dir)
setwd("../data")
# Read csv
df <- read_csv('raw_wf_demo.csv')
# 1. rcid column
# Remove since we don't need company id for only 6 companies
df <- subset(df, select = -c(rcid))
# 2. company column
# Remove ', Inc.'
df$company <- gsub(
pattern = (', Inc.'),
replacement = '',
as.character(df$company))
# Remove '\\'
df$company <- gsub(
pattern = ('\\\\'),
replacement = '',
as.character(df$company))
# 3. region column
# Fill empty values with NA
df$region <- gsub(
pattern = ('empty'),
replacement = NA,
as.character(df$region))
# 4. year column
# change 'month' column to 'year'
df$year = substr(df$month,1,nchar(df$month)-3)
df <- subset(df, select = -c(month))
# Load packages
library('tidyverse')
library('ggplot2')
library('rstudioapi')
# Set directory to data folder
cur_dir <- dirname(getSourceEditorContext()$path)
getwd()
setwd(cur_dir)
setwd("../data")
# Read csv
df <- read_csv('raw_wf_demo.csv')
# Load packages
library('tidyverse')
library('ggplot2')
library('rstudioapi')
# Set directory to data folder
cur_dir <- dirname(getSourceEditorContext()$path)
getwd()
setwd(cur_dir)
setwd("../data")
getwd()
# Read csv
df <- read_csv('raw_wf_demo.csv')
## SAVE FULL DF
getwd()
cur_dir <- dirname(getSourceEditorContext()$path)
setwd(cur_dir)
setwd("../data")
getwd()
write.csv(df, "cleaned_wf_demo.csv")
# Save sample df
# Upload sample dataframe to website
df_sam <- df[sample(nrow(df), 500), ]
write.csv(df_sam, "cleaned_wf_demo_sample.csv")

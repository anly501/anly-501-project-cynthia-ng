df <- subset(df, select = -c(month))
df <- subset(df, year == 2020)
df <- subset(df, region=='Northern America')
df <- subset(df, select = -c(year, region))
## SAVE FULL DF
cur_dir <- dirname(getSourceEditorContext()$path)
setwd(cur_dir)
setwd("../data")
write.csv(df, "cleaned_wf_demo.csv")
# Save sample df
# Upload sample dataframe to website
df_sam <- df[sample(nrow(df), 500), ]
write.csv(df_sam, "cleaned_wf_demo_sample.csv")
# Load packages
library('tidyverse')
library('ggplot2')
library('rstudioapi')
# Set directory to data folder
cur_dir <- dirname(getSourceEditorContext()$path)
getwd()
setwd(cur_dir)
setwd("../data")
getwd()
# Read csv
df <- read_csv('raw_wf_demo.csv')
# 1. rcid column
# Remove since we don't need company id for only 6 companies
df <- subset(df, select = -c(rcid))
# 2. company column
# Remove ', Inc.'
df$company <- gsub(
pattern = (', Inc.'),
replacement = '',
as.character(df$company))
# Remove '\\'
df$company <- gsub(
pattern = ('\\\\'),
replacement = '',
as.character(df$company))
# 3. region column
# Fill empty values with NA
df$region <- gsub(
pattern = ('empty'),
replacement = NA,
as.character(df$region))
# 4. year column
# change 'month' column to 'year'
df$year = substr(df$month,1,nchar(df$month)-3)
df <- subset(df, select = -c(month))
df <- subset(df, year == 2020)
df <- subset(df, region=='Northern America')
df <- subset(df, select = -c(year, region))
which(colSums(is.na(my_df))>0)
which(colSums(is.na(df))>0)
library(plyr)
# Use ddply to impute
ddply(df,
.(ethnicity),
transform,
salary=ifelse(is.na(salary),
(salary, na.rm=TRUE), salary))
# Load packages
library('tidyverse')
library('ggplot2')
library('rstudioapi')
# Set directory to data folder
cur_dir <- dirname(getSourceEditorContext()$path)
getwd()
setwd(cur_dir)
setwd("../data")
getwd()
# Read csv
df <- read_csv('raw_wf_demo.csv')
# 1. rcid column
# Remove since we don't need company id for only 6 companies
df <- subset(df, select = -c(rcid))
# 2. company column
# Remove ', Inc.'
df$company <- gsub(
pattern = (', Inc.'),
replacement = '',
as.character(df$company))
# Remove '\\'
df$company <- gsub(
pattern = ('\\\\'),
replacement = '',
as.character(df$company))
# 3. region column
# Fill empty values with NA
df$region <- gsub(
pattern = ('empty'),
replacement = NA,
as.character(df$region))
# 4. year column
# change 'month' column to 'year'
df$year = substr(df$month,1,nchar(df$month)-3)
df <- subset(df, select = -c(month))
df <- subset(df, year == 2020)
df <- subset(df, region=='Northern America')
df <- subset(df, select = -c(year, region))
# Check which columns have NAs
which(colSums(is.na(df))>0)
# It looks like the "salary" column is the only one that has NAs
# To deal with these NAs, let's impute salary values with the median value in the gender column
# Load plyr library
library(plyr)
# Use ddply to impute
ddply(df,
.(ethnicity),
transform,
salary=ifelse(is.na(salary),
(salary, na.rm=TRUE), salary))
ddply(df,
.(ethnicity),
transform,
salary=ifelse(is.na(salary), median(salary, na.rm=TRUE), salary))
which(colSums(is.na(df))>0)
ddply(df,
.ethnicity,
transform,
salary=ifelse(is.na(salary), median(salary, na.rm=TRUE), salary))
# Use ddply to impute
ddply(df,
.(ethnicity),
transform,
salary=ifelse(is.na(salary), median(salary, na.rm=TRUE), salary))
df <- ddply(df,
.(ethnicity),
transform,
salary=ifelse(is.na(salary), median(salary, na.rm=TRUE), salary))
which(colSums(is.na(df))>0)
length(df)
nrows(df)
# Load packages
library('tidyverse')
library('ggplot2')
library('rstudioapi')
# Set directory to data folder
cur_dir <- dirname(getSourceEditorContext()$path)
getwd()
setwd(cur_dir)
setwd("../data")
getwd()
# Read csv
df <- read_csv('raw_wf_demo.csv')
# 1. rcid column
# Remove since we don't need company id for only 6 companies
df <- subset(df, select = -c(rcid))
# 2. company column
# Remove ', Inc.'
df$company <- gsub(
pattern = (', Inc.'),
replacement = '',
as.character(df$company))
# Remove '\\'
df$company <- gsub(
pattern = ('\\\\'),
replacement = '',
as.character(df$company))
# 3. region column
# Fill empty values with NA
df$region <- gsub(
pattern = ('empty'),
replacement = NA,
as.character(df$region))
# 4. year column
# change 'month' column to 'year'
df$year = substr(df$month,1,nchar(df$month)-3)
df <- subset(df, select = -c(month))
df <- subset(df, year == 2020)
df <- subset(df, region=='Northern America')
df <- subset(df, select = -c(year, region))
nrow(df)
# Check which columns have NAs
which(colSums(is.na(df))>0)
# It looks like the "salary" column is the only one that has NAs
# To deal with these NAs, let's impute salary values with the median value in the gender column
# Load plyr library
library(plyr)
# Use ddply to impute
df <- ddply(df,
.(ethnicity),
transform,
salary=ifelse(is.na(salary), median(salary, na.rm=TRUE), salary))
# Confirm that there are no more NA values
which(colSums(is.na(df))>0)
nrow(df)
## SAVE FULL DF
cur_dir <- dirname(getSourceEditorContext()$path)
setwd(cur_dir)
setwd("../data")
write.csv(df, "cleaned_wf_demo.csv")
# Save sample df
# Upload sample dataframe to website
df_sam <- df[sample(nrow(df), 500), ]
write.csv(df_sam, "cleaned_wf_demo_sample.csv")
# Load relevant libraries
knitr::opts_chunk$set(echo = TRUE)
library(gmodels)
library(tidyverse)
library(ggplot2)
library(rstudioapi)
# Set directory
cur_dir <- dirname(getSourceEditorContext()$path)
setwd(cur_dir)
setwd("../data")
# Read csv
df <- read_csv("cleaned_wf_demo.csv")
# Study structure of df
str(df)
summary(df)
# Create data frame for only predictor and outcome
mydata <- data.frame(df$salary, df$ethnicity) #select ethnicity and salary
colnames(mydata) <- c('mypredictor', 'myresponse')
summary(mydata)
# Create data frame for only predictor and outcome
mydata <- data.frame(df$salary, df$ethnicity) #select ethnicity and salary
colnames(mydata) <- c('mypredictor', 'myresponse')
summary(mydata)
# Confirm that there are no more NA values
which(colSums(is.na(df))>0)
# Split
set.seed(42)
index = sample(2,nrow(mydata),prob = c(0.8,0.2),replace=TRUE) #sample training and testing sets
#training set, dataframe
train = mydata[index==1,]
#testing set, dataframe
test = mydata[index==2,]
# Check dimensions of train and test
dim(train)
dim(test)
# Check what train looks like
head(train)
# Check proportions of predictor in train and test, which should be similar
table(train$myresponse) %>% prop.table()
table(test$myresponse) %>% prop.table()
# Pull X and y from train and test
# Because I only have one X and one y variable each, I hav to group them a dataframe and name them accordingly to avoid the error of no unique columns
X_train <- data.frame(train[, names(train) %in% c("mypredictor")] )
names(X_train) <- 'mypredictor'
y_train <- train[['myresponse']]
X_test <- data.frame(test[, names(test) %in% c("mypredictor")])
names(X_test) <- 'mypredictor'
y_test <- test[['myresponse']]
# Check lengths. Lengths of X_train and y_train must equal, while lengths of X_test and y_test must equal.
length(X_train)
length(y_train)
length(X_test)
length(y_test)
# Load naive bayes library
library(naivebayes)
# Train naive bayes model
gnb <- gaussian_naive_bayes(x = data.matrix(X_train), y = y_train)
y_train_pred <- (predict(gnb, newdata = data.matrix(X_train), type = "class"))
y_test_pred <- (predict(gnb, newdata = data.matrix(X_test), type = "class"))
# Train accuracy
train_accuracy <- round((sum(y_train_pred==y_train)/length(y_test))*100 , digits = 0)
paste("Train method accuracy: " , train_accuracy, "%")
# Test accuracy
test_accuracy <- round((sum(y_test_pred==y_test)/length(y_test))*100 , digits = 0)
paste("Test method accuracy: " , test_accuracy, "%")
# Plot accuracy
accuracies_df <- data.frame (method = c('train', 'test'),
accuracy = c('77%', '20%'))
ggplot(accuracies_df, aes(x=method, y=accuracy)) +
geom_bar(stat = "identity", width=0.2) +
ggtitle("Compare accuracies") +
theme(plot.title = element_text(hjust = 0.5))
# Save plot
ggsave("../501-project-website/images/naive_bayes_r_accuracy.png")
# Load caret library
library(caret)
# Create confusion matrix
cm <- confusionMatrix(y_test_pred, as.factor(y_test))
cm
# Plot confusion matrix
ggplotConfusionMatrix <- function(m){
p <-
ggplot(data = as.data.frame(m$table) ,
aes(x = Reference, y = Prediction)) +
geom_tile(aes(fill = log(Freq)), colour = "Gray") +
scale_fill_gradient(low = "Blue", high = "Yellow") +
geom_text(aes(x = Reference, y = Prediction, label= Freq)) +
theme(legend.position = "none") +
ggtitle("Confusion matrix") + theme(plot.title = element_text(hjust = 0.5))
return(p)
}
ggplotConfusionMatrix(cm)
# Save plot
ggsave("../501-project-website/images/naive_bayes_r_confusion_matrix.png")
# Load packages
library('tidyverse')
library('ggplot2')
library('rstudioapi')
# Set directory to data folder
cur_dir <- dirname(getSourceEditorContext()$path)
getwd()
setwd(cur_dir)
setwd("../data")
getwd()
# Read csv
df <- read_csv('raw_wf_demo.csv')
# 1. rcid column
# Remove since we don't need company id for only 6 companies
df <- subset(df, select = -c(rcid))
# 2. company column
# Remove ', Inc.'
df$company <- gsub(
pattern = (', Inc.'),
replacement = '',
as.character(df$company))
# Remove '\\'
df$company <- gsub(
pattern = ('\\\\'),
replacement = '',
as.character(df$company))
# Simplify company names for better readability
# Change "The Home Depot" to "Home Depot"
colnames(df)[colnames(df) == "The Home Depot"] = "Home Depot"
colnames(df)[colnames(df) == "Bristol Myers Squibb Co."] = "Bristol Myers Squibb"
colnames(df)[colnames(df) == "Accenture Plc"] = "Accenture"
colnames(df)[colnames(df) == "The Goldman Sachs Group"] = "Goldman Sachs"
# 3. region column
# Fill empty values with NA
df$region <- gsub(
pattern = ('empty'),
replacement = NA,
as.character(df$region))
# 4. year column
# change 'month' column to 'year'
df$year = substr(df$month,1,nchar(df$month)-3)
df <- subset(df, select = -c(month))
# Set year as 2020
df <- subset(df, year == 2020)
# Set region as Northern America
df <- subset(df, region=='Northern America')
# Remove the year and region columns since they're all the same
df <- subset(df, select = -c(year, region))
# Confirm that number of rows is the same
nrow(df)
# Check which columns have NAs
which(colSums(is.na(df))>0)
# It looks like the "salary" column is the only one that has NAs
# To deal with these NAs, let's impute salary values with the median value in the gender column
# Load plyr library
library(plyr)
# Use ddply to impute
df <- ddply(df,
.(ethnicity),
transform,
salary=ifelse(is.na(salary), median(salary, na.rm=TRUE), salary))
# Confirm that there are no more NA values
which(colSums(is.na(df))>0)
# Confirm that number of rows is the same
nrow(df)
## SAVE FULL DF
cur_dir <- dirname(getSourceEditorContext()$path)
setwd(cur_dir)
setwd("../data")
write.csv(df, "cleaned_wf_demo.csv")
# Save sample df
# Upload sample dataframe to website
df_sam <- df[sample(nrow(df), 500), ]
write.csv(df_sam, "cleaned_wf_demo_sample.csv")
unique(df$company)
# Load packages
library('tidyverse')
library('ggplot2')
library('rstudioapi')
# Set directory to data folder
cur_dir <- dirname(getSourceEditorContext()$path)
getwd()
setwd(cur_dir)
setwd("../data")
getwd()
# Read csv
df <- read_csv('raw_wf_demo.csv')
knitr::opts_chunk$set(echo = TRUE)
# Load packages
library('tidyverse')
library('ggplot2')
library('rstudioapi')
# Set directory to data folder
cur_dir <- dirname(getSourceEditorContext()$path)
getwd()
setwd(cur_dir)
setwd("../data")
getwd()
# Read csv
df <- read_csv('raw_wf_demo.csv')
# 1. rcid column
# Remove since we don't need company id for only 6 companies
df <- subset(df, select = -c(rcid))
# 2. company column
# Remove ', Inc.'
df$company <- gsub(
pattern = (', Inc.'),
replacement = '',
as.character(df$company))
# Remove '\\'
df$company <- gsub(
pattern = ('\\\\'),
replacement = '',
as.character(df$company))
# Simplify company names for better readability
df$company[df$company=="The Home Depot"]<-"Home Depot"
df$company[df$company=="Bristol Myers Squibb Co."]<-"Bristol Myers Squibb"
df$company[df$company=="Accenture Plc"]<-"Accenture"
df$company[df$company=="The Goldman Sachs Group"]<-"Goldman Sachs"
# 3. region column
# Fill empty values with NA
df$region <- gsub(
pattern = ('empty'),
replacement = NA,
as.character(df$region))
# 4. year column
# change 'month' column to 'year'
df$year = substr(df$month,1,nchar(df$month)-3)
df <- subset(df, select = -c(month))
# Set year as 2020
df <- subset(df, year == 2020)
# Set region as Northern America
df <- subset(df, region=='Northern America')
# Remove the year and region columns since they're all the same
df <- subset(df, select = -c(year, region))
# Confirm that number of rows is the same
nrow(df)
# Check which columns have NAs
which(colSums(is.na(df))>0)
# It looks like the "salary" column is the only one that has NAs
# To deal with these NAs, let's impute salary values with the median value in the gender column
# Load plyr library
library(plyr)
# Use ddply to impute
df <- ddply(df,
.(ethnicity),
transform,
salary=ifelse(is.na(salary), median(salary, na.rm=TRUE), salary))
# Confirm that there are no more NA values
which(colSums(is.na(df))>0)
# Confirm that number of rows is the same
nrow(df)
## SAVE FULL DF
cur_dir <- dirname(getSourceEditorContext()$path)
setwd(cur_dir)
setwd("../data")
write.csv(df, "cleaned_wf_demo.csv")
# Save sample df
# Upload sample dataframe to website
df_sam <- df[sample(nrow(df), 500), ]
write.csv(df_sam, "cleaned_wf_demo_sample.csv")
unique(df$company)
knitr::opts_chunk$set(echo = TRUE)
# Load essential libraries
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library(tm) # for text minning
library(twitteR)
require(httr)
require(jsonlite)
require(dplyr)
# Initialize empty csv to store tweets later
api = read.csv('Gathering_Twitter_API_Keys.txt')
# Read keys and tokens
consumer_key = api[api["Type"]=="API Key"][2]
consumer_secret = api[api["Type"]=="API Key Secret"][2]
access_token = api[api["Type"]=="Access Token"][2]
access_secret = api[api["Type"]=="Access Token Secret"][2]
bearer_token = api[api["Type"]=="Bearer Token"][2]
# Read urls
requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
# Setup authentication
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
knitr::opts_chunk$set(echo = TRUE)
# Load essential libraries
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library(tm) # for text minning
library(twitteR)
require(httr)
require(jsonlite)
require(dplyr)
# Set paramters
N = 200
Lang = "en"
# Call Tweets
goldmansachs <- twitteR::searchTwitter("Goldman Sachs", n = N, lang = Lang)
knitr::opts_chunk$set(echo = TRUE)
# Load essential libraries
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library(tm) # for text minning
library(twitteR)
require(httr)
require(jsonlite)
require(dplyr)
# Initialize empty csv to store tweets later
api = read.csv('Gathering_Twitter_API_Keys.txt')
# Read keys and tokens
consumer_key = api[api["Type"]=="API Key"][2]
consumer_secret = api[api["Type"]=="API Key Secret"][2]
access_token = api[api["Type"]=="Access Token"][2]
access_secret = api[api["Type"]=="Access Token Secret"][2]
bearer_token = api[api["Type"]=="Bearer Token"][2]
# Read urls
requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
# Setup authentication
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

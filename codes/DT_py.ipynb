{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ef5cab",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04160e77",
   "metadata": {},
   "source": [
    "## Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "26e51404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          company  seniority job_category  gender ethnicity       count  \\\n",
      "1  The Home Depot          2        empty    male  multiple    1.339059   \n",
      "2  The Home Depot          2        Sales  female    native   30.866820   \n",
      "3  The Home Depot          3     Engineer  female     white  229.514520   \n",
      "4  The Home Depot          1    Scientist  female    native    1.413377   \n",
      "5  The Home Depot          2        Admin    male    native    2.650784   \n",
      "\n",
      "     inflow   outflow        salary  \n",
      "1  0.000470  0.008150  9.310531e+04  \n",
      "2  0.462686  0.081448  1.952310e+06  \n",
      "3  2.762419  5.267294  2.229767e+07  \n",
      "4  0.000212  0.009232  6.196394e+04  \n",
      "5  0.016046  0.008234  1.946259e+05  \n"
     ]
    }
   ],
   "source": [
    "# Load essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv('../data/cleaned_wf_demo.csv', index_col=0)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71e370",
   "metadata": {},
   "source": [
    "## Reorganize data for Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "209cd624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['company', 'seniority', 'count', 'inflow', 'outflow', 'salary',\n",
      "       'job_category_Admin', 'job_category_Engineer', 'job_category_Finance',\n",
      "       'job_category_Marketing', 'job_category_Operations',\n",
      "       'job_category_Sales', 'job_category_Scientist', 'job_category_empty',\n",
      "       'gender_female', 'gender_male', 'ethnicity_api', 'ethnicity_black',\n",
      "       'ethnicity_hispanic', 'ethnicity_multiple', 'ethnicity_native',\n",
      "       'ethnicity_white'],\n",
      "      dtype='object')\n",
      "company                       0\n",
      "seniority                     0\n",
      "count                         0\n",
      "inflow                        0\n",
      "outflow                       0\n",
      "salary                     2821\n",
      "job_category_Admin            0\n",
      "job_category_Engineer         0\n",
      "job_category_Finance          0\n",
      "job_category_Marketing        0\n",
      "job_category_Operations       0\n",
      "job_category_Sales            0\n",
      "job_category_Scientist        0\n",
      "job_category_empty            0\n",
      "gender_female                 0\n",
      "gender_male                   0\n",
      "ethnicity_api                 0\n",
      "ethnicity_black               0\n",
      "ethnicity_hispanic            0\n",
      "ethnicity_multiple            0\n",
      "ethnicity_native              0\n",
      "ethnicity_white               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert categorical variables to dummy variables usind OneHotEncoder\n",
    "\n",
    "# Dummies for job_category\n",
    "# Get dummies and rename new dummyies' names as job category names\n",
    "dummies = pd.get_dummies(df['job_category']).rename(columns=lambda x: 'job_category_' + str(x))\n",
    "df = pd.concat([df, dummies], axis=1) #add back into the dataframe\n",
    "df.drop(['job_category'], inplace=True, axis = 1) #remove job_category column\n",
    "\n",
    "# Repeat for gender\n",
    "dummies = pd.get_dummies(df['gender']).rename(columns=lambda x: 'gender_' + str(x))\n",
    "df = pd.concat([df, dummies], axis=1) #add back into the dataframe\n",
    "df.drop(['gender'], inplace=True, axis = 1) #remove gender column\n",
    "\n",
    "# Repeat for ethnicity\n",
    "dummies = pd.get_dummies(df['ethnicity']).rename(columns=lambda x: 'ethnicity_' + str(x))\n",
    "df = pd.concat([df, dummies], axis=1) #add back into the dataframe\n",
    "df.drop(['ethnicity'], inplace=True, axis = 1) #remove ethnicity column\n",
    "\n",
    "print(df.columns)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "88026456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0 : label = The Home Depot\n",
      "index = 1 : label = Databricks\n",
      "index = 2 : label = Bristol Myers Squibb Co.\n",
      "index = 3 : label = Accenture Plc\n",
      "index = 4 : label = Apple\n",
      "index = 5 : label = The Goldman Sachs Group\n",
      "(42336, 21) (42336,)\n"
     ]
    }
   ],
   "source": [
    "## SET X AND Y\n",
    "\n",
    "# y: CONVERT FROM STRING LABELS TO INTEGERS \n",
    "labels=[]; \n",
    "y=[]\n",
    "for label in df[\"company\"]:\n",
    "    if label not in labels:\n",
    "        labels.append(label)\n",
    "        print(\"index =\",len(labels)-1,\": label =\",label)\n",
    "    for i in range(0,len(labels)):\n",
    "        if(label==labels[i]):\n",
    "            y.append(i)\n",
    "y = np.array(y)\n",
    "\n",
    "# X\n",
    "X = df.drop(['company'], axis = 1)\n",
    "\n",
    "# Double check\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5051af2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33868, 21)\n",
      "(33868,)\n",
      "(8468, 21)\n",
      "(8468,)\n"
     ]
    }
   ],
   "source": [
    "# Load sklearn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Partion data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=4)\n",
    "\n",
    "# Check \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c1fc7",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d914ee48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "\n",
    "# Import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Fit model\n",
    "fit1 = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "fit1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9a022949",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/cynthiang/Sync/School/Georgetown/Class/501-Intro/portfolio/anly-501-project-cynthia-ng/codes/DT_py.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cynthiang/Sync/School/Georgetown/Class/501-Intro/portfolio/anly-501-project-cynthia-ng/codes/DT_py.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Fit to training data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cynthiang/Sync/School/Georgetown/Class/501-Intro/portfolio/anly-501-project-cynthia-ng/codes/DT_py.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m sklearn\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cynthiang/Sync/School/Georgetown/Class/501-Intro/portfolio/anly-501-project-cynthia-ng/codes/DT_py.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m fit1\u001b[39m.\u001b[39;49mfit(X_train, y_train) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cynthiang/Sync/School/Georgetown/Class/501-Intro/portfolio/anly-501-project-cynthia-ng/codes/DT_py.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# USE ONE-HOT ENCODER TO CONVERT STRING TO FLOAT\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cynthiang/Sync/School/Georgetown/Class/501-Intro/portfolio/anly-501-project-cynthia-ng/codes/DT_py.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Predict on X_train\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cynthiang/Sync/School/Georgetown/Class/501-Intro/portfolio/anly-501-project-cynthia-ng/codes/DT_py.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y_train_pred \u001b[39m=\u001b[39m fit1\u001b[39m.\u001b[39mpredict(X_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/class/lib/python3.10/site-packages/sklearn/tree/_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    940\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \n\u001b[1;32m    942\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    970\u001b[0m         X,\n\u001b[1;32m    971\u001b[0m         y,\n\u001b[1;32m    972\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    973\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    974\u001b[0m     )\n\u001b[1;32m    975\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/class/lib/python3.10/site-packages/sklearn/tree/_classes.py:172\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    170\u001b[0m check_X_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39mDTYPE, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m check_y_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 172\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    173\u001b[0m     X, y, validate_separately\u001b[39m=\u001b[39;49m(check_X_params, check_y_params)\n\u001b[1;32m    174\u001b[0m )\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[1;32m    176\u001b[0m     X\u001b[39m.\u001b[39msort_indices()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/class/lib/python3.10/site-packages/sklearn/base.py:591\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_X_params:\n\u001b[1;32m    590\u001b[0m     check_X_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_X_params}\n\u001b[0;32m--> 591\u001b[0m X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_X_params)\n\u001b[1;32m    592\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_y_params:\n\u001b[1;32m    593\u001b[0m     check_y_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/class/lib/python3.10/site-packages/sklearn/utils/validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    894\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    895\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    896\u001b[0m         )\n\u001b[1;32m    898\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 899\u001b[0m         _assert_all_finite(\n\u001b[1;32m    900\u001b[0m             array,\n\u001b[1;32m    901\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    902\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    903\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    904\u001b[0m         )\n\u001b[1;32m    906\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/class/lib/python3.10/site-packages/sklearn/utils/validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    125\u001b[0m             \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[1;32m    126\u001b[0m             \u001b[39mand\u001b[39;00m estimator_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m             msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    133\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m             )\n\u001b[0;32m--> 146\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n\u001b[1;32m    148\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Fit to training data\n",
    "from sklearn.utils.fixes import sklearn\n",
    "\n",
    "fit1.fit(X_train, y_train) \n",
    "\n",
    "# USE ONE-HOT ENCODER TO CONVERT STRING TO FLOAT\n",
    "# Predict on X_train\n",
    "y_train_pred = fit1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf0b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST ACCURACY\n",
    "# Training set\n",
    "print(\"Training set\")\n",
    "print(\"Accuracy: \", accuracy_score(y_train, y_train_pred) * 100) #accuracy score\n",
    "print(\"Number of mislabeled points out of a total 4275 points: \", (y_train != y_train_pred).sum()) #mislabeled points\n",
    "\n",
    "# Test set\n",
    "y_test_pred = fit1.predict(X_test)\n",
    "print(\"Test set\")\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_test_pred)*100) #accuracy score\n",
    "print(\"Number of mislabeled points out of a total 1069 points: \", (y_test != y_test_pred).sum()) #mislabeled points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc8712",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f18630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracies = pd.DataFrame({'Set':['Training set','Test set'], 'Accuracy (%)': [accuracy_score(y_train, y_train_pred) * 100, accuracy_score(y_test, y_test_pred)*100]})\n",
    "sns.barplot(data=model_accuracies, x=\"Set\", y=\"Accuracy (%)\").set(title = 'Accuracy of model for training vs test sets' )\n",
    "\n",
    "# Save plot\n",
    "plt.savefig(\"../501-project-website/images/DT_tweets_accuracy.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae17468",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(fit1 , X_test ,  y_test ,  cmap=\"Blues\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.title(\"Confusion matrix for Decision Tree model\")\n",
    "\n",
    "# Save confusion matrix plot\n",
    "plt.savefig(\"../501-project-website/images/DT_tweets_confusion_matrix.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('class')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "15746aa1dcf7a2a78475ba9215d3f8835d2f92e9634eaaa8b18bbade37f57527"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
